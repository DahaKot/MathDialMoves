{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf3b067-95e1-4b7a-9873-82ec6ad78193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nThe following is a fragment of a conversatio...</td>\n",
       "      <td>probing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nThe following is a fragment of a conversatio...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nThe following is a fragment of a conversatio...</td>\n",
       "      <td>probing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThe following is a fragment of a conversatio...</td>\n",
       "      <td>probing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nThe following is a fragment of a conversatio...</td>\n",
       "      <td>telling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  \\nThe following is a fragment of a conversatio...  probing\n",
       "1  \\nThe following is a fragment of a conversatio...  generic\n",
       "2  \\nThe following is a fragment of a conversatio...  probing\n",
       "3  \\nThe following is a fragment of a conversatio...  probing\n",
       "4  \\nThe following is a fragment of a conversatio...  telling"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries needed for data preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Download the dataset and put it in subfolder called data\n",
    "datapath = \"train_only_dialogue_window_1.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Show the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4674ef-4fd3-4bda-9829-2f71cb5ef460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of news: 12646\n",
      "----------------------------------------\n",
      "Split by category:\n",
      "label\n",
      "focus      5334\n",
      "probing    3005\n",
      "telling    2428\n",
      "generic    1879\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Number of categories: 4\n"
     ]
    }
   ],
   "source": [
    "print('Total number of news: {}'.format(len(df)))\n",
    "print(40*'-')\n",
    "print('Split by category:')\n",
    "print(df[\"label\"].value_counts())\n",
    "print(40*'-')\n",
    "nr_categories = len(df[\"label\"].unique())\n",
    "print(\"Number of categories: {n}\".format(n=nr_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "510b9a84-7a1e-4480-aa66-c9b10da6536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 ... 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "X = df['text']\n",
    "y=np.unique(df['label'], return_inverse=True)[1]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7396146b-c8c8-46fb-8301-ce16262b1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb5dcee-3837-4e57-82f9-4021b449c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_list=X.to_list()\n",
    "X_pt = tokenizer(X_list, padding='max_length', max_length = 512, truncation=True, return_tensors='pt')[\"input_ids\"]\n",
    "\n",
    "y_list=y.tolist()\n",
    "y_pt = torch.Tensor(y_list).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8819d0-e0d8-4c1c-817b-7781abfd7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath_test = \"test_only_dialogue_window_1.csv\"\n",
    "df_test = pd.read_csv(datapath_test)\n",
    "df_test = df_test[[\"text\", \"label\"]]\n",
    "\n",
    "X_test = df_test['text']\n",
    "y_test=np.unique(df_test['label'], return_inverse=True)[1]\n",
    "\n",
    "X_list_test=X_test.to_list()\n",
    "X_pt_test = tokenizer(X_list_test, padding='max_length', max_length = 512, truncation=True, return_tensors='pt')[\"input_ids\"]\n",
    "\n",
    "y_list_test=y_test.tolist()\n",
    "y_pt_test = torch.Tensor(y_list_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301e10b7-70d9-43c0-be44-f3b9f7e87945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch dataset\n",
    "\n",
    "X_pt_train = X_pt\n",
    "y_pt_train = y_pt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class BBCNewsDataset(Dataset):\n",
    "    \"\"\"Custom-built BBC News dataset\"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X, y as Torch tensors\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_train[idx], self.y_train[idx]# Get train and test data in form of Dataset class\n",
    "train_data_pt = BBCNewsDataset(X=X_pt_train, y=y_pt_train)\n",
    "test_data_pt = BBCNewsDataset(X=X_pt_test, y=y_pt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f96d0221-771a-4832-9e80-0cf14f6d0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test data in form of Dataloader class\n",
    "train_loader_pt = DataLoader(train_data_pt, batch_size=50, shuffle=True)\n",
    "test_loader_pt = DataLoader(test_data_pt, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e21c9404-cfcd-47d0-bb2c-2dc72fba52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "dbert_pt = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec1a36af-a0d9-43ea-8e12-817732f571d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type:  <class 'transformers.modeling_outputs.BaseModelOutput'>\n",
      "Output format (shape):  torch.Size([5, 512, 768])\n",
      "Output used as input for the classifier (shape):  torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "# Let's create a sample of size 5 from the training data\n",
    "sample = X_pt_train[0:5]\n",
    "print('Object type: ', type(dbert_pt(sample)))\n",
    "print('Output format (shape): ',dbert_pt(sample)[0].shape)\n",
    "print('Output used as input for the classifier (shape): ', dbert_pt(sample)[0][:,0,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59e6f344-e96e-4765-9b8b-7dc70de8b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class DistilBertClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBertClassification, self).__init__()\n",
    "        self.dbert = dbert_pt\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear1 = nn.Linear(768,64)\n",
    "        self.ReLu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(64,5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dbert(input_ids=x)\n",
    "        x = x[\"last_hidden_state\"][:,0,:]\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.ReLu(x)\n",
    "        logits = self.linear2(x)\n",
    "        # No need for a softmax, because it is already included in the CrossEntropyLoss\n",
    "        return logits\n",
    "\n",
    "model_pt = DistilBertClassification().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "383187b2-8d7d-467d-b132-241d1696a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_pt.dbert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fadc445-6c26-4184-b080-c3becf8c5893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  66412421\n",
      "Number of trainable parameters:  49541\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model_pt.parameters())\n",
    "total_params_trainable = sum(p.numel() for p in model_pt.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: \", total_params)\n",
    "print(\"Number of trainable parameters: \", total_params_trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7819d155-a8ce-46a1-bae4-15bbdb19e630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/253 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.62 GiB of which 86.19 MiB is free. Process 2627555 has 22.36 GiB memory in use. Including non-PyTorch memory, this process has 774.00 MiB memory in use. Of the allocated memory 490.85 MiB is allocated by PyTorch, and 85.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m# Loop on batches\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m tqdm(train_loader_pt):\n\u001b[1;32m     29\u001b[0m     \u001b[39m# Get prediction & loss\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     prediction \u001b[39m=\u001b[39m model_pt(X\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     31\u001b[0m     loss \u001b[39m=\u001b[39m criterion(prediction, y\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     33\u001b[0m     \u001b[39m# Adjust the parameters of the model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mDistilBertClassification.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbert(input_ids\u001b[39m=\u001b[39mx)\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m x[\u001b[39m\"\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m\"\u001b[39m][:,\u001b[39m0\u001b[39m,:]\n\u001b[1;32m     18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:822\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m         attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(input_shape, device\u001b[39m=\u001b[39mdevice)  \u001b[39m# (bs, seq_length)\u001b[39;00m\n\u001b[0;32m--> 822\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    823\u001b[0m     x\u001b[39m=\u001b[39membeddings,\n\u001b[1;32m    824\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    825\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m    826\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    827\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    828\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    829\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:587\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    579\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    580\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    581\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m         output_attentions,\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    586\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    588\u001b[0m         hidden_state,\n\u001b[1;32m    589\u001b[0m         attn_mask,\n\u001b[1;32m    590\u001b[0m         head_mask[i],\n\u001b[1;32m    591\u001b[0m         output_attentions,\n\u001b[1;32m    592\u001b[0m     )\n\u001b[1;32m    594\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:513\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[1;32m    514\u001b[0m     query\u001b[39m=\u001b[39mx,\n\u001b[1;32m    515\u001b[0m     key\u001b[39m=\u001b[39mx,\n\u001b[1;32m    516\u001b[0m     value\u001b[39m=\u001b[39mx,\n\u001b[1;32m    517\u001b[0m     mask\u001b[39m=\u001b[39mattn_mask,\n\u001b[1;32m    518\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m    519\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    520\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    522\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:240\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    238\u001b[0m q \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_lin(query))  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m k \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_lin(key))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m v \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_lin(value))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    242\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    243\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 23.62 GiB of which 86.19 MiB is free. Process 2627555 has 22.36 GiB memory in use. Including non-PyTorch memory, this process has 774.00 MiB memory in use. Of the allocated memory 490.85 MiB is allocated by PyTorch, and 85.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_pt.parameters())\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Define the dictionary \"history\" that will collect key performance indicators during training\n",
    "history = {}\n",
    "history[\"epoch\"]=[]\n",
    "history[\"train_loss\"]=[]\n",
    "history[\"valid_loss\"]=[]\n",
    "history[\"train_accuracy\"]=[]\n",
    "history[\"valid_accuracy\"]=[]\n",
    "\n",
    "from datetime import datetime\n",
    "# Measure time for training\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Loop on epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # Set mode in train mode\n",
    "    model_pt.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_accuracy = []\n",
    "    \n",
    "    # Loop on batches\n",
    "    for X, y in tqdm(train_loader_pt):\n",
    "        # Get prediction & loss\n",
    "        prediction = model_pt(X.to(device))\n",
    "        loss = criterion(prediction, y.to(device))\n",
    "        \n",
    "        # Adjust the parameters of the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        prediction_index = prediction.argmax(axis=1)\n",
    "        accuracy = (prediction_index==y.to(device))\n",
    "        train_accuracy += accuracy\n",
    "    \n",
    "    train_accuracy = (sum(train_accuracy) / len(train_accuracy)).item()\n",
    "    \n",
    "    # Calculate the loss on the test data after each epoch\n",
    "    # Set mode to evaluation (by opposition to training)\n",
    "    model_pt.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_accuracy = []\n",
    "    for X, y in tqdm(test_loader_pt):\n",
    "        \n",
    "        prediction = model_pt(X.to(device))\n",
    "        loss = criterion(prediction, y.to(device))\n",
    "\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "        prediction_index = prediction.argmax(axis=1)\n",
    "        accuracy = (prediction_index==y.to(device))\n",
    "        valid_accuracy += accuracy\n",
    "    valid_accuracy = (sum(valid_accuracy) / len(valid_accuracy)).item()\n",
    "    \n",
    "    # Populate history\n",
    "    history[\"epoch\"].append(e+1)\n",
    "    history[\"train_loss\"].append(train_loss / len(train_loader_pt))\n",
    "    history[\"valid_loss\"].append(valid_loss / len(test_loader_pt))\n",
    "    history[\"train_accuracy\"].append(train_accuracy)\n",
    "    history[\"valid_accuracy\"].append(valid_accuracy)    \n",
    "        \n",
    "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_loader_pt) :10.3f} \\t\\t Validation Loss: {valid_loss / len(test_loader_pt) :10.3f}')\n",
    "    print(f'\\t\\t Training Accuracy: {train_accuracy :10.3%} \\t\\t Validation Accuracy: {valid_accuracy :10.3%}')\n",
    "    \n",
    "# Measure time for training\n",
    "end_time = datetime.now()\n",
    "training_time_pt = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed12e9-0c07-40d6-82bc-331678077ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "ax[0].set(title='Loss')\n",
    "ax[0].plot(history['train_loss'], label='Training')\n",
    "ax[0].plot(history['valid_loss'], label='Validation')\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "\n",
    "ax[1].set(title='Accuracy')\n",
    "ax[1].plot(history['train_accuracy'], label='Training')\n",
    "ax[1].plot(history['valid_accuracy'], label='Validation')\n",
    "ax[1].legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e9b21-2b67-4090-b987-1c286bec3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_pt = history['valid_accuracy'][-1]\n",
    "print('Accuracy Training data: {:.1%}'.format(history['train_accuracy'][-1]))\n",
    "print('Accuracy Test data: {:.1%}'.format(history['valid_accuracy'][-1]))\n",
    "print('Training time: {:.1f}s (or {:.1f} minutes)'.format(training_time_pt, training_time_pt/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd86737-95af-40c2-9c1f-c7d2bf1efeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the parameters of the model but not the model itself, and get it back\n",
    "torch.save(model_pt.state_dict(), 'PyModel_window_1.sd')\n",
    "model_reloaded = DistilBertClassification()\n",
    "model_reloaded.load_state_dict(torch.load('PyModel_window_1.sd'))\n",
    "model_reloaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271ad8f-818b-4390-bfa2-9019f11be1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model, and get it back\n",
    "torch.save(model_pt, 'PyModelComplete_window_1.pt')\n",
    "model_reloaded2 = torch.load('PyModelComplete_window_1.pt')\n",
    "model_reloaded2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b618a-e772-4e8e-bd0b-338b2dd60ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction = model_pt(X_pt_test[:5].to(device)).argmax(axis=1)\n",
    "report = classification_report(y_pt_test[:5], prediction.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b888014-3dfa-4f87-acef-4608048437a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c34b7ed6-ffa1-4a84-a945-0dcdf8893dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 12646\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"./data/train_window_1.csv\", \"test\": \"./data/test_window_1.csv\"})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37019089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (RobertaForMultipleChoice, RobertaTokenizer, Trainer,\n",
    "                          TrainingArguments, XLMRobertaForMultipleChoice,\n",
    "                          XLMRobertaTokenizer, RobertaForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d55477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training roberta-base on DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 12646\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 3100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "logging_dir = \"./logs/roberta_base/window_1\"\n",
    "\n",
    "print(f\"Training {model_name} on {dataset}\")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "if model_name == \"roberta-base\":\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "elif model_name == \"xlm-roberta-base\":\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "else:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    print(\"Using the default roberta tokenizer, be careful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057b0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    texts, labels = examples[\"text\"], examples[\"label\"]\n",
    "\n",
    "    # Tokenize premises and choices\n",
    "    # Note that we provide both choices together as multiple_choices_inputs\n",
    "    multiple_choices_inputs = []\n",
    "    for text in texts:\n",
    "        multiple_choices_inputs.append(tokenizer.encode_plus( \\\n",
    "            text, max_length=512, padding='max_length', \\\n",
    "            truncation=True))\n",
    "\n",
    "    # RoBERTa expects a list of all first choices and a list of all second \n",
    "    # choices, hence we restructure the inputs\n",
    "    input_ids = [x['input_ids'] for x in multiple_choices_inputs]\n",
    "    attention_masks = [x['attention_mask'] for x in multiple_choices_inputs]\n",
    "\n",
    "    labels = np.unique(labels, return_inverse=True)[1]\n",
    "    print(type(torch.tensor(attention_masks).view(-1, 512)), torch.tensor(attention_masks).view(-1, 512).dtype)\n",
    "\n",
    "    # Restructure inputs to match the expected format for RobertaForMultipleChoice\n",
    "    features = {\n",
    "        'input_ids': torch.tensor(input_ids).view(-1, 512),\n",
    "        'attention_mask': torch.tensor(attention_masks).view(-1, 512),\n",
    "        'labels': torch.tensor(labels)\n",
    "    }\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db002c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fa616e1b5b4745bede8efe552b81cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfd2179fb8b46c1a519886ed8910920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n",
      "<class 'torch.Tensor'> torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Map the preprocessing function over the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6d3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"index\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e9ffa97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_datasets[\"test\"][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949f0589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"roberta-base\":\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=4).to(device)\n",
    "elif model_name == \"xlm-roberta-base\":\n",
    "    model = XLMRobertaForMultipleChoice.from_pretrained(model_name).to(device)\n",
    "else:\n",
    "    model = RobertaForMultipleChoice.from_pretrained(model_name).to(device)\n",
    "    print(\"Using the default roberta, be careful\")\n",
    "\n",
    "output_dir = f\"{logging_dir}/{model_name}\"\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.07, \n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-6, \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    report_to = \"wandb\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0029e4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12646\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127efddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a52d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001302d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc3c78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaha_kot\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daria.kotova/nlp804/MathDialMoves/wandb/run-20240419_175601-yg10ac7a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daha_kot/huggingface/runs/yg10ac7a' target=\"_blank\">ancient-fire-110</a></strong> to <a href='https://wandb.ai/daha_kot/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daha_kot/huggingface' target=\"_blank\">https://wandb.ai/daha_kot/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daha_kot/huggingface/runs/yg10ac7a' target=\"_blank\">https://wandb.ai/daha_kot/huggingface/runs/yg10ac7a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cbcc6c82fe4ecdbe8d99f373c559e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4076, 'learning_rate': 2.2593764121102575e-08, 'epoch': 0.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7609b6e5ce374c9fa4ab935d0f605f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4059182405471802, 'eval_accuracy': 0.1406451612903226, 'eval_runtime': 17.1024, 'eval_samples_per_second': 181.262, 'eval_steps_per_second': 22.687, 'epoch': 0.03}\n",
      "{'loss': 1.4051, 'learning_rate': 4.518752824220515e-08, 'epoch': 0.06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ed5630d792437f88fc7bd77a5476a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4051610231399536, 'eval_accuracy': 0.1406451612903226, 'eval_runtime': 16.8076, 'eval_samples_per_second': 184.44, 'eval_steps_per_second': 23.085, 'epoch': 0.06}\n",
      "{'loss': 1.4108, 'learning_rate': 6.778129236330772e-08, 'epoch': 0.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0802a94b505f40dfb2872b59ee41a253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.403479814529419, 'eval_accuracy': 0.1406451612903226, 'eval_runtime': 16.7911, 'eval_samples_per_second': 184.621, 'eval_steps_per_second': 23.107, 'epoch': 0.09}\n",
      "{'loss': 1.4039, 'learning_rate': 9.03750564844103e-08, 'epoch': 0.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31547e9b5c0f4ff080990c395642cd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4012691974639893, 'eval_accuracy': 0.1406451612903226, 'eval_runtime': 16.9846, 'eval_samples_per_second': 182.518, 'eval_steps_per_second': 22.844, 'epoch': 0.13}\n",
      "{'loss': 1.4024, 'learning_rate': 1.1296882060551286e-07, 'epoch': 0.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41075429fa1e4525b37f03f9cc655149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3992811441421509, 'eval_accuracy': 0.1406451612903226, 'eval_runtime': 16.8946, 'eval_samples_per_second': 183.491, 'eval_steps_per_second': 22.966, 'epoch': 0.16}\n",
      "{'loss': 1.3905, 'learning_rate': 1.3556258472661544e-07, 'epoch': 0.19}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dff00e239e4403b99b7b2ec01f4213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3963583707809448, 'eval_accuracy': 0.1406451612903226, 'eval_runtime': 16.8416, 'eval_samples_per_second': 184.068, 'eval_steps_per_second': 23.038, 'epoch': 0.19}\n",
      "{'loss': 1.3906, 'learning_rate': 1.5815634884771803e-07, 'epoch': 0.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414a6eeab8af43209f543181390f3d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3933305740356445, 'eval_accuracy': 0.1406451612903226, 'eval_runtime': 16.8207, 'eval_samples_per_second': 184.297, 'eval_steps_per_second': 23.067, 'epoch': 0.22}\n",
      "{'loss': 1.3834, 'learning_rate': 1.807501129688206e-07, 'epoch': 0.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18698364c0d4d929bf17633fe2f2552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3890458345413208, 'eval_accuracy': 0.1406451612903226, 'eval_runtime': 16.8255, 'eval_samples_per_second': 184.244, 'eval_steps_per_second': 23.06, 'epoch': 0.25}\n",
      "{'loss': 1.3923, 'learning_rate': 2.033438770899232e-07, 'epoch': 0.28}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7503610ddc143c49add90c99424764c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3852674961090088, 'eval_accuracy': 0.14193548387096774, 'eval_runtime': 16.7491, 'eval_samples_per_second': 185.085, 'eval_steps_per_second': 23.165, 'epoch': 0.28}\n",
      "{'loss': 1.3842, 'learning_rate': 2.2593764121102573e-07, 'epoch': 0.32}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9be53a92814a82affac793f7846570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3810272216796875, 'eval_accuracy': 0.23806451612903226, 'eval_runtime': 16.7439, 'eval_samples_per_second': 185.142, 'eval_steps_per_second': 23.173, 'epoch': 0.32}\n",
      "{'loss': 1.3729, 'learning_rate': 2.485314053321283e-07, 'epoch': 0.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f856be4e75bf4aa0b2a47b9d4335f191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3758118152618408, 'eval_accuracy': 0.3841935483870968, 'eval_runtime': 16.7436, 'eval_samples_per_second': 185.145, 'eval_steps_per_second': 23.173, 'epoch': 0.35}\n",
      "{'loss': 1.3675, 'learning_rate': 2.711251694532309e-07, 'epoch': 0.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31ea418f1cb4cd7a47ed27f5b56b5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3687165975570679, 'eval_accuracy': 0.38451612903225807, 'eval_runtime': 16.813, 'eval_samples_per_second': 184.381, 'eval_steps_per_second': 23.077, 'epoch': 0.38}\n",
      "{'loss': 1.3509, 'learning_rate': 2.9371893357433345e-07, 'epoch': 0.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9cf10d566e47c89edc17129422a6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3577444553375244, 'eval_accuracy': 0.38451612903225807, 'eval_runtime': 16.8747, 'eval_samples_per_second': 183.707, 'eval_steps_per_second': 22.993, 'epoch': 0.41}\n",
      "{'loss': 1.338, 'learning_rate': 3.1631269769543607e-07, 'epoch': 0.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b801da5c4e5a4a9c8d404fd6afc7cbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.342713475227356, 'eval_accuracy': 0.38451612903225807, 'eval_runtime': 16.8894, 'eval_samples_per_second': 183.548, 'eval_steps_per_second': 22.973, 'epoch': 0.44}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aecfea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'text', 'label', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12646\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0dd258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaha_kot\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daria.kotova/nlp804/MathDialMoves/wandb/run-20240418_224736-jcyjrupz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daha_kot/huggingface/runs/jcyjrupz' target=\"_blank\">happy-universe-107</a></strong> to <a href='https://wandb.ai/daha_kot/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daha_kot/huggingface' target=\"_blank\">https://wandb.ai/daha_kot/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daha_kot/huggingface/runs/jcyjrupz' target=\"_blank\">https://wandb.ai/daha_kot/huggingface/runs/jcyjrupz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4095013a9248c6a148477d181bc1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     29\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     30\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     31\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     40\u001b[0m \u001b[39m# Path where the checkpoints are saved\u001b[39;00m\n\u001b[1;32m     41\u001b[0m checkpoints_path \u001b[39m=\u001b[39m output_dir\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/trainer.py:1836\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1835\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1836\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1837\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1839\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/data_loader.py:451\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    452\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/data/data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[1;32m     93\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/data/data_collator.py:141\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    139\u001b[0m     label \u001b[39m=\u001b[39m first[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(first[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m], torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m first[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    140\u001b[0m     dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(label, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mfloat\n\u001b[0;32m--> 141\u001b[0m     batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([f[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features], dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    142\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m first \u001b[39mand\u001b[39;00m first[\u001b[39m\"\u001b[39m\u001b[39mlabel_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(first[\u001b[39m\"\u001b[39m\u001b[39mlabel_ids\u001b[39m\u001b[39m\"\u001b[39m], torch\u001b[39m.\u001b[39mTensor):\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Path where the checkpoints are saved\n",
    "checkpoints_path = output_dir\n",
    "checkpoints = [os.path.join(checkpoints_path, name) \\\n",
    "                for name in os.listdir(checkpoints_path) \\\n",
    "                if name.startswith(\"checkpoint\")]\n",
    "\n",
    "# Placeholder for the best performance\n",
    "best_performance = 0.0\n",
    "best_checkpoint = None\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    # Load the model from checkpoint\n",
    "    if model_name == \"roberta-base\":\n",
    "        model = RobertaForMultipleChoice.from_pretrained(checkpoint).to(device)\n",
    "    elif model_name == \"xlm-roberta-base\":\n",
    "        model = XLMRobertaForMultipleChoice.from_pretrained(checkpoint).to(device)\n",
    "    else:\n",
    "        model = RobertaForMultipleChoice.from_pretrained(checkpoint).to(device)\n",
    "        print(\"Using the default roberta, be careful\")\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_eval_batch_size=1,  # Adjust as necessary\n",
    "        ),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate(tokenized_datasets['validation'])\n",
    "\n",
    "    # Assuming 'accuracy' is your metric of interest\n",
    "    print(eval_results)\n",
    "    performance = eval_results[\"eval_accuracy\"]\n",
    "\n",
    "    # Update the best checkpoint if current model is better\n",
    "    if performance > best_performance:\n",
    "        best_performance = performance\n",
    "        best_checkpoint = checkpoint\n",
    "\n",
    "print(f\"Best checkpoint: {best_checkpoint} with Eval Loss: {best_performance}\")\n",
    "\n",
    "if best_checkpoint:\n",
    "    print(f\"Best checkpoint: {best_checkpoint} with Eval Loss: {best_performance}\")\n",
    "\n",
    "    # Load the best model\n",
    "    if model_name == \"roberta-base\":\n",
    "        best_model = RobertaForMultipleChoice.from_pretrained(best_checkpoint).to(device)\n",
    "    elif model_name == \"xlm-roberta-base\":\n",
    "        best_model = XLMRobertaForMultipleChoice.from_pretrained(best_checkpoint).to(device)\n",
    "    else:\n",
    "        best_model = RobertaForMultipleChoice.from_pretrained(best_checkpoint).to(device)\n",
    "        print(\"Using the default roberta, be careful\")\n",
    "\n",
    "    # Directly save the best model to the desired directory\n",
    "    best_model.save_pretrained(f\"{output_dir}/best_{best_checkpoint}\")\n",
    "\n",
    "    # If you want to save the tokenizer as well\n",
    "    tokenizer.save_pretrained(f\"{output_dir}/best_{best_checkpoint}\")\n",
    "\n",
    "    # Optional: Evaluate the best model again for confirmation, using the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=best_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=f'./{output_dir}/best',  # Ensure this matches where you're saving the model\n",
    "            per_device_eval_batch_size=8,\n",
    "        ),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    eval_results = trainer.evaluate(tokenized_datasets['test'])\n",
    "    print(\"Final Evaluation on Best Model:\", eval_results)\n",
    "else:\n",
    "    print(\"No best checkpoint identified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f54f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
