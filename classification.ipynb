{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf3b067-95e1-4b7a-9873-82ec6ad78193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nThe following is a fragment of a conversatio...</td>\n",
       "      <td>probing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nThe following is a fragment of a conversatio...</td>\n",
       "      <td>probing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nThe following is a fragment of a conversatio...</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThe following is a fragment of a conversatio...</td>\n",
       "      <td>probing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nThe following is a fragment of a conversatio...</td>\n",
       "      <td>probing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  \\nThe following is a fragment of a conversatio...  probing\n",
       "1  \\nThe following is a fragment of a conversatio...  probing\n",
       "2  \\nThe following is a fragment of a conversatio...  generic\n",
       "3  \\nThe following is a fragment of a conversatio...  probing\n",
       "4  \\nThe following is a fragment of a conversatio...  probing"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries needed for data preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Download the dataset and put it in subfolder called data\n",
    "datapath = \"train_only_dialogue_window_1.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Show the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4674ef-4fd3-4bda-9829-2f71cb5ef460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of news: 14908\n",
      "----------------------------------------\n",
      "Split by category:\n",
      "focus      5549\n",
      "generic    3566\n",
      "probing    3300\n",
      "telling    2493\n",
      "Name: label, dtype: int64\n",
      "----------------------------------------\n",
      "Number of categories: 4\n"
     ]
    }
   ],
   "source": [
    "print('Total number of news: {}'.format(len(df)))\n",
    "print(40*'-')\n",
    "print('Split by category:')\n",
    "print(df[\"label\"].value_counts())\n",
    "print(40*'-')\n",
    "nr_categories = len(df[\"label\"].unique())\n",
    "print(\"Number of categories: {n}\".format(n=nr_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "510b9a84-7a1e-4480-aa66-c9b10da6536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 1 ... 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "X = df['text']\n",
    "y=np.unique(df['label'], return_inverse=True)[1]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7396146b-c8c8-46fb-8301-ce16262b1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb5dcee-3837-4e57-82f9-4021b449c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_list=X.to_list()\n",
    "X_pt = tokenizer(X_list, padding='max_length', max_length = 512, truncation=True, return_tensors='pt')[\"input_ids\"]\n",
    "\n",
    "y_list=y.tolist()\n",
    "y_pt = torch.Tensor(y_list).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8819d0-e0d8-4c1c-817b-7781abfd7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath_test = \"test_only_dialogue_window_1.csv\"\n",
    "df_test = pd.read_csv(datapath_test)\n",
    "df_test = df_test[[\"text\", \"label\"]]\n",
    "\n",
    "X_test = df_test['text']\n",
    "y_test=np.unique(df_test['label'], return_inverse=True)[1]\n",
    "\n",
    "X_list_test=X_test.to_list()\n",
    "X_pt_test = tokenizer(X_list_test, padding='max_length', max_length = 512, truncation=True, return_tensors='pt')[\"input_ids\"]\n",
    "\n",
    "y_list_test=y_test.tolist()\n",
    "y_pt_test = torch.Tensor(y_list_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301e10b7-70d9-43c0-be44-f3b9f7e87945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch dataset\n",
    "\n",
    "X_pt_train = X_pt\n",
    "y_pt_train = y_pt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class BBCNewsDataset(Dataset):\n",
    "    \"\"\"Custom-built BBC News dataset\"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X, y as Torch tensors\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_train[idx], self.y_train[idx]# Get train and test data in form of Dataset class\n",
    "train_data_pt = BBCNewsDataset(X=X_pt_train, y=y_pt_train)\n",
    "test_data_pt = BBCNewsDataset(X=X_pt_test, y=y_pt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f96d0221-771a-4832-9e80-0cf14f6d0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test data in form of Dataloader class\n",
    "train_loader_pt = DataLoader(train_data_pt, batch_size=50, shuffle=True)\n",
    "test_loader_pt = DataLoader(test_data_pt, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e21c9404-cfcd-47d0-bb2c-2dc72fba52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "dbert_pt = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec1a36af-a0d9-43ea-8e12-817732f571d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type:  <class 'transformers.modeling_outputs.BaseModelOutput'>\n",
      "Output format (shape):  torch.Size([5, 512, 768])\n",
      "Output used as input for the classifier (shape):  torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "# Let's create a sample of size 5 from the training data\n",
    "sample = X_pt_train[0:5]\n",
    "print('Object type: ', type(dbert_pt(sample)))\n",
    "print('Output format (shape): ',dbert_pt(sample)[0].shape)\n",
    "print('Output used as input for the classifier (shape): ', dbert_pt(sample)[0][:,0,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59e6f344-e96e-4765-9b8b-7dc70de8b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class DistilBertClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBertClassification, self).__init__()\n",
    "        self.dbert = dbert_pt\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear1 = nn.Linear(768,64)\n",
    "        self.ReLu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(64,5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dbert(input_ids=x)\n",
    "        x = x[\"last_hidden_state\"][:,0,:]\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.ReLu(x)\n",
    "        logits = self.linear2(x)\n",
    "        # No need for a softmax, because it is already included in the CrossEntropyLoss\n",
    "        return logits\n",
    "\n",
    "model_pt = DistilBertClassification().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "383187b2-8d7d-467d-b132-241d1696a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_pt.dbert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fadc445-6c26-4184-b080-c3becf8c5893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  66412421\n",
      "Number of trainable parameters:  49541\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model_pt.parameters())\n",
    "total_params_trainable = sum(p.numel() for p in model_pt.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: \", total_params)\n",
    "print(\"Number of trainable parameters: \", total_params_trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819d155-a8ce-46a1-bae4-15bbdb19e630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 299/299 [48:32<00:00,  9.74s/it]\n",
      "100%|███████████████████████████████████████████| 74/74 [11:39<00:00,  9.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss:      1.274 \t\t Validation Loss:      1.287\n",
      "\t\t Training Accuracy:    44.580% \t\t Validation Accuracy:    44.850%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 299/299 [48:30<00:00,  9.73s/it]\n",
      "100%|███████████████████████████████████████████| 74/74 [11:40<00:00,  9.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 \t\t Training Loss:      1.242 \t\t Validation Loss:      1.242\n",
      "\t\t Training Accuracy:    46.512% \t\t Validation Accuracy:    44.688%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                        | 11/299 [01:47<46:54,  9.77s/it]"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_pt.parameters())\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Define the dictionary \"history\" that will collect key performance indicators during training\n",
    "history = {}\n",
    "history[\"epoch\"]=[]\n",
    "history[\"train_loss\"]=[]\n",
    "history[\"valid_loss\"]=[]\n",
    "history[\"train_accuracy\"]=[]\n",
    "history[\"valid_accuracy\"]=[]\n",
    "\n",
    "from datetime import datetime\n",
    "# Measure time for training\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Loop on epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # Set mode in train mode\n",
    "    model_pt.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_accuracy = []\n",
    "    \n",
    "    # Loop on batches\n",
    "    for X, y in tqdm(train_loader_pt):\n",
    "        # Get prediction & loss\n",
    "        prediction = model_pt(X.to(device))\n",
    "        loss = criterion(prediction, y.to(device))\n",
    "        \n",
    "        # Adjust the parameters of the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        prediction_index = prediction.argmax(axis=1)\n",
    "        accuracy = (prediction_index==y.to(device))\n",
    "        train_accuracy += accuracy\n",
    "    \n",
    "    train_accuracy = (sum(train_accuracy) / len(train_accuracy)).item()\n",
    "    \n",
    "    # Calculate the loss on the test data after each epoch\n",
    "    # Set mode to evaluation (by opposition to training)\n",
    "    model_pt.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_accuracy = []\n",
    "    for X, y in tqdm(test_loader_pt):\n",
    "        \n",
    "        prediction = model_pt(X.to(device))\n",
    "        loss = criterion(prediction, y.to(device))\n",
    "\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "        prediction_index = prediction.argmax(axis=1)\n",
    "        accuracy = (prediction_index==y.to(device))\n",
    "        valid_accuracy += accuracy\n",
    "    valid_accuracy = (sum(valid_accuracy) / len(valid_accuracy)).item()\n",
    "    \n",
    "    # Populate history\n",
    "    history[\"epoch\"].append(e+1)\n",
    "    history[\"train_loss\"].append(train_loss / len(train_loader_pt))\n",
    "    history[\"valid_loss\"].append(valid_loss / len(test_loader_pt))\n",
    "    history[\"train_accuracy\"].append(train_accuracy)\n",
    "    history[\"valid_accuracy\"].append(valid_accuracy)    \n",
    "        \n",
    "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_loader_pt) :10.3f} \\t\\t Validation Loss: {valid_loss / len(test_loader_pt) :10.3f}')\n",
    "    print(f'\\t\\t Training Accuracy: {train_accuracy :10.3%} \\t\\t Validation Accuracy: {valid_accuracy :10.3%}')\n",
    "    \n",
    "# Measure time for training\n",
    "end_time = datetime.now()\n",
    "training_time_pt = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed12e9-0c07-40d6-82bc-331678077ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "ax[0].set(title='Loss')\n",
    "ax[0].plot(history['train_loss'], label='Training')\n",
    "ax[0].plot(history['valid_loss'], label='Validation')\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "\n",
    "ax[1].set(title='Accuracy')\n",
    "ax[1].plot(history['train_accuracy'], label='Training')\n",
    "ax[1].plot(history['valid_accuracy'], label='Validation')\n",
    "ax[1].legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e9b21-2b67-4090-b987-1c286bec3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_pt = history['valid_accuracy'][-1]\n",
    "print('Accuracy Training data: {:.1%}'.format(history['train_accuracy'][-1]))\n",
    "print('Accuracy Test data: {:.1%}'.format(history['valid_accuracy'][-1]))\n",
    "print('Training time: {:.1f}s (or {:.1f} minutes)'.format(training_time_pt, training_time_pt/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd86737-95af-40c2-9c1f-c7d2bf1efeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the parameters of the model but not the model itself, and get it back\n",
    "torch.save(model_pt.state_dict(), 'PyModel_window_1.sd')\n",
    "model_reloaded = DistilBertClassification()\n",
    "model_reloaded.load_state_dict(torch.load('PyModel_window_1.sd'))\n",
    "model_reloaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271ad8f-818b-4390-bfa2-9019f11be1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model, and get it back\n",
    "torch.save(model_pt, 'PyModelComplete_window_1.pt')\n",
    "model_reloaded2 = torch.load('PyModelComplete_window_1.pt')\n",
    "model_reloaded2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b618a-e772-4e8e-bd0b-338b2dd60ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction = model_pt(X_pt_test[:5].to(device)).argmax(axis=1)\n",
    "report = classification_report(y_pt_test[:5], prediction.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b888014-3dfa-4f87-acef-4608048437a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b7ed6-ffa1-4a84-a945-0dcdf8893dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
