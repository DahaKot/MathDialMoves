{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Load sentences from the text file\n",
    "def load_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    sentences = [line.strip().split(': ')[1] for line in lines]\n",
    "    return sentences\n",
    "\n",
    "# Calculate BLEU score\n",
    "def calculate_bleu_scores(file_sentences, df_sentences):\n",
    "    # references = [[ref.lower().split()] for ref in df_sentences]\n",
    "    # hypotheses = [[sent.lower().split()] for sent in file_sentences]\n",
    "    # bleu_score = sentence_bleu(references, hypotheses)\n",
    "\n",
    "    bleu_scores = []\n",
    "    for ref, sent in tqdm(zip(df_sentences, file_sentences)):\n",
    "        try:\n",
    "            bleu_scores.append(sentence_bleu(ref.lower().split(), sent, smoothing_function=SmoothingFunction().method4))\n",
    "        except:\n",
    "            print(\"problem\")\n",
    "\n",
    "    bleu_score = np.array(bleu_scores).mean()\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load DataFrame\n",
    "# Assuming df_sentences is your DataFrame column containing the sentences\n",
    "test_data = pd.read_csv(\"./data/test_replics.csv\")\n",
    "def remove_teacher(text):\n",
    "    return text[10:]\n",
    "\n",
    "test_data = test_data[\"text\"].apply(remove_teacher)\n",
    "\n",
    "# Path to your text file\n",
    "sentences_file = 'predicted_answers_without_label.txt'  # Change 'your_sentences.txt' to your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Okay -  you've overcomplicated this. Let's sta...\n",
       "1       No, you need to add as she still has the spoon...\n",
       "2                                        Yes, that's it\\n\n",
       "3       Okay, let's start again. If each bid adds $50 ...\n",
       "4       Yes $150. Carmen adds a $50 bid after each of ...\n",
       "                              ...                        \n",
       "3095                 3*60+40+70 = 180+40+70=290 minutes\\n\n",
       "3096    That doesn't seem to be correct. Can you find ...\n",
       "3097    Great. The question now says that 1/5th of tha...\n",
       "3098    That's wonderful. So now what do you think how...\n",
       "3099            There's your answer. I hope you get it.\\n\n",
       "Name: text, Length: 3100, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Hi Mariana, thank you for sharing your solutio...\n",
      "1       Good job on calculating the number of spoons J...\n",
      "2       Great job! You got it. Julia had a total of 15...\n",
      "3       Hi Ayisha, thank you for sharing your thought ...\n",
      "4       That's correct. Now, let's add this additional...\n",
      "                              ...                        \n",
      "3095    Actually, the correct total amount of minutes ...\n",
      "3096    Thank you for sharing your solution. Let's go ...\n",
      "3097    That's correct. 225 cars have valid tickets. N...\n",
      "3098    That's correct. Now, can you calculate how man...\n",
      "3099    Great job! Your final answer of 30 cars withou...\n",
      "Name: Utterance, Length: 3100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def read_file_to_dataframe(file_path):\n",
    "    data = []\n",
    "    current_index = None\n",
    "    current_utterance = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Check if the line starts with an index pattern (number followed by a colon)\n",
    "            if re.match(r'^\\d+:', line):\n",
    "                # If there's an existing utterance, save it before starting a new one\n",
    "                if current_index is not None:\n",
    "                    data.append([current_index, ''.join(current_utterance).strip()])\n",
    "                # Start a new utterance\n",
    "                current_index, rest_of_line = line.split(':', 1)\n",
    "                current_utterance = [rest_of_line]\n",
    "            else:\n",
    "                # Continue accumulating lines for the current utterance\n",
    "                current_utterance.append(line)\n",
    "    \n",
    "    # Add the last utterance to the data if it exists\n",
    "    if current_index is not None:\n",
    "        data.append([current_index, ''.join(current_utterance).strip()])\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Index', 'Utterance'])\n",
    "    return df\n",
    "\n",
    "# Specify the path to your file\n",
    "file_path = 'predicted_answers_without_label.txt'\n",
    "df = read_file_to_dataframe(file_path)\n",
    "print(df[\"Utterance\"])\n",
    "\n",
    "file_sentences = df[\"Utterance\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1906it [00:05, 302.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem\n",
      "problem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2611it [00:08, 325.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem\n",
      "problem\n",
      "problem\n",
      "problem\n",
      "problem\n",
      "problem\n",
      "problem\n",
      "problem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3100it [00:09, 315.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem\n",
      "problem\n",
      "problem\n",
      "Average BLEU score: 0.050780423353342305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU score\n",
    "avg_bleu_score = calculate_bleu_scores(list(file_sentences), list(test_data))\n",
    "\n",
    "# Print average BLEU score\n",
    "print(f\"Average BLEU score: {avg_bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3bd2942a304516bf8fc0fb5f9bb2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f471c552f90b4d76bd6d032939d0e8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8.64 seconds, 358.65 sentences/sec\n",
      "torch.Size([3100])\n",
      "0.8470321893692017\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bert_score import score\n",
    "\n",
    "# Calculate BERTScore\n",
    "def calculate_bert_scores(file_sentences, df_sentences):\n",
    "    _, _, bert_scores = score(file_sentences, df_sentences, lang=\"en\", verbose=True)\n",
    "    print(bert_scores.shape)\n",
    "    avg_bert_score = bert_scores.mean().item()\n",
    "    return avg_bert_score\n",
    "\n",
    "    # Calculate BERTScore\n",
    "\n",
    "avg_bert_score = calculate_bert_scores(list(file_sentences), list(test_data))\n",
    "\n",
    "print(avg_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
